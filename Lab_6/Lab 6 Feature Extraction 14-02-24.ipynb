{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9acc8d29-70ef-44be-91d2-93f739b2f0c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "## Cheatsheet\n",
    "\n",
    "https://www.datacamp.com/cheat-sheet/pyspark-cheat-sheet-spark-dataframes-in-python\n",
    "\n",
    "https://github.com/PacktPublishing/PySpark-Cookbook/tree/master\n",
    "\n",
    "https://runawayhorse001.github.io/LearningApacheSpark/pyspark.pdf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7914276-c782-4c90-b529-6f89875a712b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "some_text = spark.createDataFrame([\n",
    "    ['''\n",
    "    Apache Spark achieves high performance for both batch\n",
    "    and streaming data, using a state-of-the-art DAG scheduler, \n",
    "    a query optimizer, and a physical execution engine.\n",
    "    ''']\n",
    "    , ['''\n",
    "    Apache Spark is a fast and general-purpose cluster computing \n",
    "    system. It provides high-level APIs in Java, Scala, Python \n",
    "    and R, and an optimized engine that supports general execution \n",
    "    graphs. It also supports a rich set of higher-level tools including \n",
    "    Spark SQL for SQL and structured data processing, MLlib for machine \n",
    "    learning, GraphX for graph processing, and Spark Streaming.\n",
    "    ''']\n",
    "    , ['''\n",
    "    Machine learning is a field of computer science that often uses \n",
    "    statistical techniques to give computers the ability to \"learn\" \n",
    "    (i.e., progressively improve performance on a specific task) \n",
    "    with data, without being explicitly programmed.\n",
    "    ''']\n",
    "], ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656c5003-ca7e-4056-bfbb-b8b9e0333a34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "splitter = feat.RegexTokenizer(\n",
    "    inputCol='text'\n",
    "    , outputCol='text_split'\n",
    "    , pattern='\\s+|[,.\\\"]'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "401d7035-aee4-4501-9da0-d6302a74cec6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: [Row(text_split=['machine', 'learning', '(ml)', 'is', 'a', 'field', 'of', 'study', 'in', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'development', 'and', 'study', 'of', 'statistical', 'algorithms', 'that', 'can', 'learn', 'from', 'data', 'and', 'generalize', 'to', 'unseen', 'data', 'and', 'thus', 'perform', 'tasks', 'without', 'explicit', 'instructions', '[1]', 'recently', 'generative', 'artificial', 'neural', 'networks', 'have', 'been', 'able', 'to', 'surpass', 'many', 'previous', 'approaches', 'in', 'performance'])]"
     ]
    }
   ],
   "source": [
    "splitter.transform(some_text).select('text_split').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ba0e5fc-6dd6-4a05-9cae-d8e148308ae1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sw_remover = feat.StopWordsRemover(\n",
    "    inputCol=splitter.getOutputCol()\n",
    "    , outputCol='no_stopWords'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2963417-dfc1-4fd4-b323-63b27ade16e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: [Row(no_stopWords=['machine', 'learning', '(ml)', 'field', 'study', 'artificial', 'intelligence', 'concerned', 'development', 'study', 'statistical', 'algorithms', 'learn', 'data', 'generalize', 'unseen', 'data', 'thus', 'perform', 'tasks', 'without', 'explicit', 'instructions', '[1]', 'recently', 'generative', 'artificial', 'neural', 'networks', 'able', 'surpass', 'many', 'previous', 'approaches', 'performance'])]"
     ]
    }
   ],
   "source": [
    "sw_remover.transform(splitter.transform(some_text)).select('no_stopWords').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00726b59-7dc8-4fc8-8e52-476033d482bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hasher = feat.HashingTF(\n",
    "    inputCol=sw_remover.getOutputCol()\n",
    "    , outputCol='hashed'\n",
    "    , numFeatures=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1511594-f2e5-4c0a-a26a-e304de2bd120",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: [Row(hashed=SparseVector(20, {0: 3.0, 1: 1.0, 3: 2.0, 4: 3.0, 5: 2.0, 6: 2.0, 7: 1.0, 8: 1.0, 10: 1.0, 11: 3.0, 12: 3.0, 13: 1.0, 14: 5.0, 15: 2.0, 16: 3.0, 17: 1.0, 18: 1.0}))]"
     ]
    }
   ],
   "source": [
    "hasher.transform(sw_remover.transform(splitter.transform(some_text))).select('hashed').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb2e2d85-40d7-4cfd-9184-9a66f63c9b24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "idf = feat.IDF(\n",
    "    inputCol=hasher.getOutputCol()\n",
    "    , outputCol='features'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3495124c-3b96-4431-8aa6-6677eb928e40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "idfModel = idf.fit(hasher.transform(sw_remover.transform(splitter.transform(some_text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9368e06-90c9-478c-85cf-3edfea176261",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: [Row(features=SparseVector(20, {0: 0.0, 1: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 1.2164, 17: 0.0, 18: 0.4055}))]"
     ]
    }
   ],
   "source": [
    "idfModel.transform(hasher.transform(sw_remover.transform(splitter.transform(some_text)))).select('features').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3656754f-e634-475d-9930-788246ffc32a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79faee9c-592d-4b9e-9690-f8ef89787086",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: [Row(text='Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.', features=SparseVector(20, {0: 0.0, 1: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 10: 0.0, 11: 0.0, 12: 0.0, 13: 0.0, 14: 0.0, 15: 0.0, 16: 1.2164, 17: 0.0, 18: 0.4055}))]"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[splitter, sw_remover, hasher, idf])\n",
    "\n",
    "pipelineModel = pipeline.fit(some_text)\n",
    "pipelineModel.transform(some_text).select('text','features').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb6659d-c915-46df-9944-247cffc59ff5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w2v = feat.Word2Vec(\n",
    "    vectorSize=5\n",
    "    , minCount=2\n",
    "    , inputCol=sw_remover.getOutputCol()\n",
    "    , outputCol='vector'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aee611e-a584-42cf-97cd-376a7d75578b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[21]: [Row(vector=DenseVector([0.0033, -0.001, 0.0013, -0.0091, -0.0094]))]"
     ]
    }
   ],
   "source": [
    "model=w2v.fit(sw_remover.transform(splitter.transform(some_text)))\n",
    "model.transform(sw_remover.transform(splitter.transform(some_text))).select('vector').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41df564a-9817-438b-97da-5e01242abae5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lab 6 Feature Extraction 14-02-24",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
